#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
check_race_cancellation.py - ãƒ¬ãƒ¼ã‚¹é–‹å‚¬ä¸­æ­¢æƒ…å ±ã®å–å¾—

netkeibaã‹ã‚‰é–‹å‚¬ä¸­æ­¢æƒ…å ±ã‚’å–å¾—ã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã™ã‚‹
"""

import sys
import requests
import re
from datetime import datetime
from zoneinfo import ZoneInfo
from bs4 import BeautifulSoup


def http_get(url: str, timeout=20) -> str:
    """HTTP GET ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=timeout)
    r.raise_for_status()
    return r.text


def check_cancellation_news(ymd: str) -> dict:
    """
    netkeibaã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é–‹å‚¬ä¸­æ­¢æƒ…å ±ã‚’å–å¾—
    
    Args:
        ymd (str): å¯¾è±¡æ—¥ä»˜ï¼ˆYYYYMMDDï¼‰
    
    Returns:
        dict: é–‹å‚¬ä¸­æ­¢æƒ…å ±
    """
    # netkeibaã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸
    url = "https://news.netkeiba.com/?pid=news_list"
    
    try:
        html = http_get(url)
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ¤œç´¢
        articles = soup.find_all('div', class_='news_list')
        
        for article in articles[:20]:  # æœ€æ–°20ä»¶ã‚’ç¢ºèª
            title_elem = article.find('a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            link = title_elem.get('href', '')
            
            # ã€Œé–‹å‚¬ä¸­æ­¢ã€ã€Œä¸­æ­¢ã€ã€Œå–ã‚Šã‚„ã‚ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
            cancellation_keywords = ['é–‹å‚¬ä¸­æ­¢', 'ä¸­æ­¢', 'å–ã‚Šã‚„ã‚', 'å–ã‚Šæ­¢ã‚', 'é–‹å‚¬å–ã‚Šã‚„ã‚']
            
            if any(keyword in title for keyword in cancellation_keywords):
                # æ—¥ä»˜ã‚’ç¢ºèªï¼ˆè¨˜äº‹å†…ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ï¼‰
                date_match = re.search(r'(\d+)æ—¥', title)
                
                if date_match:
                    day = int(date_match.group(1))
                    target_day = int(ymd[6:8])
                    
                    # æ—¥ä»˜ãŒä¸€è‡´ã™ã‚‹å ´åˆ
                    if day == target_day:
                        # ç†ç”±ã‚’æŠ½å‡ºï¼ˆé›ªãƒ»å°é¢¨ãƒ»é¦¬å ´ä¸è‰¯ãªã©ï¼‰
                        reason = "å¤©å€™ä¸è‰¯"
                        if 'é›ª' in title or 'ç©é›ª' in title:
                            reason = "é›ªã®ãŸã‚"
                        elif 'å°é¢¨' in title:
                            reason = "å°é¢¨ã®ãŸã‚"
                        elif 'é¦¬å ´' in title:
                            reason = "é¦¬å ´ä¸è‰¯ã®ãŸã‚"
                        
                        # ç«¶é¦¬å ´ã‚’æŠ½å‡º
                        venues = []
                        venue_keywords = ['æ±äº¬', 'äº¬éƒ½', 'é˜ªç¥', 'ä¸­å±±', 'å°å€‰', 'æ–°æ½Ÿ', 'ç¦å³¶', 'ä¸­äº¬', 'æœ­å¹Œ', 'å‡½é¤¨']
                        for venue in venue_keywords:
                            if venue in title:
                                venues.append(venue)
                        
                        return {
                            "is_cancelled": True,
                            "reason": reason,
                            "venues": venues if venues else ["å…¨ç«¶é¦¬å ´"],
                            "title": title,
                            "link": f"https://news.netkeiba.com{link}" if link.startswith('/') else link,
                            "date": ymd
                        }
        
        return {"is_cancelled": False, "date": ymd}
        
    except Exception as e:
        print(f"âš ï¸ é–‹å‚¬ä¸­æ­¢æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
        return {"is_cancelled": False, "date": ymd, "error": str(e)}


def check_race_list_page(ymd: str) -> dict:
    """
    ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å‚¬ä¸­æ­¢æƒ…å ±ã‚’å–å¾—
    
    Args:
        ymd (str): å¯¾è±¡æ—¥ä»˜ï¼ˆYYYYMMDDï¼‰
    
    Returns:
        dict: é–‹å‚¬ä¸­æ­¢æƒ…å ±
    """
    url = f"https://race.netkeiba.com/top/race_list.html?kaisai_date={ymd}"
    
    try:
        html = http_get(url)
        soup = BeautifulSoup(html, 'html.parser')
        
        # ã€Œé–‹å‚¬ä¸­æ­¢ã€ã€Œä¸­æ­¢ã€ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
        page_text = soup.get_text()
        
        if 'é–‹å‚¬ä¸­æ­¢' in page_text or 'ä¸­æ­¢' in page_text:
            # ä¸­æ­¢æƒ…å ±ã‚’æŠ½å‡º
            cancellation_info = soup.find(string=re.compile(r'é–‹å‚¬ä¸­æ­¢|ä¸­æ­¢'))
            
            if cancellation_info:
                parent = cancellation_info.find_parent()
                if parent:
                    info_text = parent.get_text(strip=True)
                    
                    # ç†ç”±ã‚’æŠ½å‡º
                    reason = "å¤©å€™ä¸è‰¯"
                    if 'é›ª' in info_text or 'ç©é›ª' in info_text:
                        reason = "é›ªã®ãŸã‚"
                    elif 'å°é¢¨' in info_text:
                        reason = "å°é¢¨ã®ãŸã‚"
                    elif 'é¦¬å ´' in info_text:
                        reason = "é¦¬å ´ä¸è‰¯ã®ãŸã‚"
                    
                    return {
                        "is_cancelled": True,
                        "reason": reason,
                        "info": info_text,
                        "date": ymd
                    }
        
        return {"is_cancelled": False, "date": ymd}
        
    except Exception as e:
        print(f"âš ï¸ ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã®ç¢ºèªã«å¤±æ•—: {e}")
        return {"is_cancelled": False, "date": ymd, "error": str(e)}


def main():
    if len(sys.argv) < 2:
        print("Usage: python check_race_cancellation.py YYYYMMDD")
        sys.exit(1)
    
    ymd = sys.argv[1]
    
    print(f"ğŸ“… å¯¾è±¡æ—¥ä»˜: {ymd}")
    print("=" * 60)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ç¢ºèª
    print("\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é–‹å‚¬ä¸­æ­¢æƒ…å ±ã‚’ç¢ºèªä¸­...")
    news_result = check_cancellation_news(ymd)
    
    if news_result.get('is_cancelled'):
        print(f"âœ… é–‹å‚¬ä¸­æ­¢ã‚’æ¤œå‡º:")
        print(f"   ç†ç”±: {news_result['reason']}")
        print(f"   ç«¶é¦¬å ´: {', '.join(news_result['venues'])}")
        print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {news_result['title']}")
        print(f"   ãƒªãƒ³ã‚¯: {news_result['link']}")
        
        import json
        output = {
            "date": ymd,
            "is_cancelled": True,
            "reason": news_result['reason'],
            "venues": news_result['venues'],
            "source": "news",
            "title": news_result['title'],
            "link": news_result['link']
        }
        
        with open(f"cancellation_info_{ymd}.json", "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… cancellation_info_{ymd}.json ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return 0
    
    # ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ç¢ºèª
    print("\nğŸ” ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ç¢ºèªä¸­...")
    list_result = check_race_list_page(ymd)
    
    if list_result.get('is_cancelled'):
        print(f"âœ… é–‹å‚¬ä¸­æ­¢ã‚’æ¤œå‡º:")
        print(f"   ç†ç”±: {list_result['reason']}")
        print(f"   æƒ…å ±: {list_result.get('info', '')}")
        
        import json
        output = {
            "date": ymd,
            "is_cancelled": True,
            "reason": list_result['reason'],
            "source": "race_list",
            "info": list_result.get('info', '')
        }
        
        with open(f"cancellation_info_{ymd}.json", "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… cancellation_info_{ymd}.json ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return 0
    
    # é–‹å‚¬ä¸­æ­¢ãªã—
    print("\nâœ… é–‹å‚¬ä¸­æ­¢ã®æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    import json
    output = {
        "date": ymd,
        "is_cancelled": False
    }
    
    with open(f"cancellation_info_{ymd}.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… cancellation_info_{ymd}.json ã‚’ä½œæˆã—ã¾ã—ãŸ")
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
